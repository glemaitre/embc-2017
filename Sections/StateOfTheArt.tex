\section{Denoising Algorithms} \label{sc:methodology}
%%We can add kind of intro to this section


%\subsection{Traditionnal filters}

In this paper, a set of conventional filters are used: (i) a mean filter, (ii) a median filter, (iii) a local statistics filter (i.e. Lee filter~\cite{4766994}), (iv) hard and soft thresholding in wavelet domain~\cite{wavelet}, and (v) \ac{nlm}~\cite{nlm}. The more recent techniques used are described below.

%Filter Given a noisy image $g$, the restored value on the denoised image $\hat{f}$ at the position $(x, y)$ corresponds to the average of the neighbourhood $N$ (See Eq. \ref{eq:mean-filter}).

%\begin{equation}
	%\hat{f}(x, y) = \frac{1}{M\cdot N}\sum_{(u, v) \in N(x, y)}{g(x, y)}.
    %\label{eq:mean-filter}
%\end{equation}

%This process acts on the supposition that the noise is concentrated on the upper part of the frequency spectrum. This approach is able to remove pixels which are not representative in the considered neighborhood and also reduce the noise by blurring the image. Thus, high frequencies are lost during the process.

%This is the simplest denoising technique and it is not  very effective (but was tested as a comparison). Denoising through a mean filter simply applies a linear transformation of the input image  (a convolution with a smoothing kernel), and it makes no attempt to interpret the information in the image and use it in the denoising process.


%\subsection{Median filter}

%Median filter is a spatial filter based on order-statistics. It replaces the intensity value at each position with the 50th percentile of the neighborhood (See Eq. \ref{eq:median-filter}). 

%\begin{equation}
	%\hat{f}(x, y) = {median}_{\substack{(u, v) \in N(x, y)}}{g(x, y)}.
    %\label{eq:median-filter}
%\end{equation}

%Unlike mean filter, median filter is able to detect outliers of a neighborhood and remove them with a smaller impact on the higher frequencies. For the same reason, this filter is providing good results onto  images affected by salt-and-pepper noise. 

%The main drawback of this method is its computational complexity for large groups of data. % What? No!

%subsection{Filtering by use of local statistics (LS filter)}
%Digital Image enhancement by using local statistics is a computational technique that involves contrast and noise filtering on two-dimensional arrays based on their local mean and variance. One of the greatest advantages of this type of algorithms is that they are non-recursive and each pixel is processed independently. As a consequence this approach has a great advantage when  used in real time image processing.
%The assumption of the algorithm based on local statistics is that the sample mean and variance of a pixel is equal to the local mean and variance of all the pixels within a fixed range.

%This simple approach has been pointed as to lack mathematical elegance and sophistication, compared to other techniques, however the results indicate it is a very effective tool for contrast stretching and noise filtering of images.

%et $x_{ij}$ be the brightness of the pixel $(i,j)$ in a two dimensional $N\times N$ image. The local mean and variance are then calculated over a $(2n+1) \times (2m+1)$ window. The local mean is defined as:

%\begin{equation}
    %\mu_{ij}=\dfrac{1}{(2n+1)(2m+1)}\sum_{k=i-n}^{n+i}\sum_{l=j-m}^{m+j}{x_{kl}},
    %\label{eq:ls_filter_1}
%\end{equation}

%and the local variance is:

%\begin{equation}
   % v_{j} =\dfrac{1}{(2n+1)(2m+1)} \sum_{k=i-n}^{i+m}\sum_{l=j-m}^{j+m} (x_{kl} - \mu_{ij} )^2.
    %\label{eq:ls_filter_2}
%\end{equation}

%From these equations it is not hard to extend the algorithm to deal with images corrupted by additive or multiplicative noise or even both. A noisy corrupted image is described as: 

%\begin{equation}
%z_{ij} = x_{ij}*u_{ij} + w_{ij}.
    %\label{eq:ls_filter_3}
%\end{equation}

%Where the mean and variance are calculated as:

%\begin{equation}
    %E[(u_{ij} - \vec{u_{ij}})( u_{kl} -\vec{u_{kl}})] = \sigma^2*\delta_{ik}*\delta_{jl}.
    %\label{eq:ls_filter_4}
%\end{equation}

%From the structure of the algorithm, it is easy to see that the principal computational load relays on the calculation of the local mean and variance of the image. To make the calculations faster, an improvement to the algorithm is proposed where the image is partitioned in square sub regions over which the local variance and mean are calculated. Further, the local mean and variance of a pixel are approximated by the use of two dimensional interpolation formulas. This improvement seems to be promising and perfectly suitable for real time -parallel image processing.

%\subsection{Hard and soft thresholding in wavelet domain (wavelet filter)}


%The wavelet transform is used extensively in signal de-noising. The usual way to de-noise signals in wavelet domain is to first transform the signal into wavelet domain, apply hard or soft thresholding and then transform back. 

%Hard thresholding is a noise suppression method, that applies the following transformation to the empirical wavelet coefficients:

%\begin{equation}
   % F(x)=x\cdot I(|x|>t),
   %\label{eq:wavelet_1}
%\end{equation}

%where $t$ is a threshold value. For de-noising to perform adequately, $t$ must be chosen carefully.

%The theoretically optimal value for $t$ is $t=\sqrt{2\sigma^{2}log(n)/n}$, where $\sigma^{2}$is the variance of the noise and $n$ is the length of input data. In practice, usually, a smaller value is usually used ~\cite{thresholding}.

%Soft thresholding, just like hard thresholding, incorporates a transformation of the empirical wavelet coefficients. The only difference is the chosen nonlinear transformation:

%\begin{equation}
    %S(x)=sign(x)(|x|-t)\cdot I(|x|>t),
    %\label{eq:wavelet_2}
%\end{equation}

%where, again, $t$ is the threshold value.

%However, when the signal contains discontinuities, the denoising will also result in artifacts: pseudo-Gibbs phenomena, when the signal is alternatively overshooting or undershooting its level. These artifacts depend on the precise alignment between the signal and the basis elements, therefore depend both on wavelets and the input data. 

%A solution was proposed in "Translation-Invariant De-Noising"~\cite{wavelet}, where Coifman and Donoho present an algorithm to minimize the effects of this phenomenon. 



\subsection{\acf{ssdc} approach}
This technique is based on the paper of Yahya~\emph{et~al.}~\cite{6717020}; first, the speckle noise is converted from multiplicative to additive via homomorphic filtering and later the vector space of the noisy image is decomposed into signal and noise subspaces.
The enhancement occurs by canceling the noise subspsace and estimating through a linear estimator the clean image from the remaining signal subspace. 

\subsection{\acf{bm3d}}
\ac{bm3d} is an image denoising strategy which uses block matching and collaborative filtering in the 3D domain~\cite{dabov2007image}.
The core algorithm is composed of three steps: (i) grouping, (ii) collaborative filtering, and (iii) aggregation.
The first step consists in grouping similar 2D image patches from different spatial locations, to form 3D blocks.
The collaborative filtering is equivalent to denoise the 3D blocks by successively applying a 3D transform, a denoising method, and an inverse 3D transform.
Finally, a denoised image is reconstructed by making a linear combination of the 2D denoised patches.

The previous algorithm is applied twice in the \ac{bm3d} framework to build: (i) a basic estimate and (ii) a final estimate.
More precisely, the basic estimate is computed by grouping noisy 2D patches, denoising the blocks via hard-thresholding, and aggregating the patches by setting the weights to be inversely proportional to the total sample variance of the blocks.
Then, the grouping in the final estimate is built from two distinct blocks by arranging 2D patches from both the noisy image and basic estimate.
The filtering is performed through a Wiener filter driven by the blocks extracted from the basic estimate, considered as the true energy spectrum.
The aggregation step is equivalent to the one performed in the basic estimate stage to obtain the final denoised image.


%The aim of block matching is to stack similar 2D image fragments ("blocks"), in 3D arrays called "groups".
%Similarities are computed between candidate fragments at different spatial locations and each reference fragment.
%The groups are disjoints, so some blocks can be stacked in multiple groups.
%These groups have a "diameter" corresponding to the maximum number of blocks that they contained.

%After getting the groups by block matching, a collaborative filtering is applied, which include: 3D transformation, shrinkage of the transform spectrum, and inverse 3D transform.
%Different 3D transformations can be applied according to the type of noise, or it can be decomposed in 2D transform followed by 1D transform.

%The last process used is the aggregation.
%Because the groups are disjoints, multiple estimates are given for some blocks so that they will overlap.
%In order to aggregate them, the blocks are awarded with weights.
%For each final pixel, the block pixels correspond to the average with their given weights.

%The algorithm is divided in two steps.
%The first one consists in applying block matching and collaborative filtering on the noisy image.
%The shrinkage of the coefficients of the 3D group is realised for this first step with an hard-thresholding filter.
%The basic estimate got from the first step helps to improves an other block matching in the second step.
%Collaborative filtering is applied once again on these new obtained blocks and the shrinkage is now realised by a Wiener filter, which attenuates the frequencies using %the signal to noise ratio.
%The final estimate is therefore obtained, what corresponds to the denoised image.

%Multiple parameters can be tuned in order to choose between a faster or more accurate algorithm, such as: block size, group diameter, step between reference blocks, and search area.

%This method improves the \ac{nlm} filter method~\cite{nlm} in using 3D transform instead of 1D.
%Thanks to the filtering in transform domain applied on the already process groups, the method preserves uniform areas, smooth intensity transitions, textures, repeating patterns, and sharp edges.
%The main advantages of this approach is the non-locality and the collaborative filtering

\subsection{K-SVD} \label{sc:description-ksvd}
This denoising method consists in decomposing images using sparse and redundant representations over trained dictionaries.
The authors in~\cite{ksvd} propose two possible implementations: (i) with a pre-trained dictionary learnt from high quality natural images and using the \ac{dct}, or (ii) by training a new dictionary using the corrupted one.
We chose the first option with an overcomplete \ac{dct} dictionary since that this option leads to better performance.

%\subsection{NLM}

%Non local means (NL-means) algorithm for image denoising in ~\cite{nlm} is based on a non-local averaging of all pixels in the image. The main difference of the NL-means algorithm compared to local filters or frequency domain filters is the systematic use of all possible self-predictions the image can provide, a principle used also in ~\cite{texture}.  

%Non local means algorithm is based on the following equation:

%\begin{equation}
%	\begin{split}
%		NL&[u](x)=\\
 %       &\frac{1}{C(x)}\int_{\Omega} e^{-\frac{(G_a*\mid u(x+.)-u(y+.)\mid ^2(o))}{h^2}}u(y)dy,
%	\end{split}
%\end{equation}
%where $x \epsilon\, \in\,  \Omega$,
%\begin{equation}
%	\begin{split}
%	    C(x)&= \\
 %       &\int_{\Omega} e^{-\frac{(G_a*\mid u(x+.)-u(y+.)\mid ^2(o))}{h^2}}u(y)dy
  %  \end{split}
%\end{equation}
%is a normalizing constant, $h$ is the filtering parameter and $G_a$ is a Gaussian kernel. The new value of a pixel $x$ is defined as the mean of all the pixels in the image whose neighborhood is similar to the neighborhood of $x$.

%\begin{equation}
%	NL[v](i)=\sum_{j\space\epsilon\space I}{w(i,j)\cdot v(i,j)},
%\end{equation}

%where $v= \{v(i)  \mid  i \in I \}$ represents all the pixels in an Image and the weights for each pixel depend on the similarity between the two pixels I and j. The weights {w(i,j)}, depend on the similarity between two pixels I and j under the conditions: $0 < w(i,j)< 1,\,\sum_{j}{w(i,j)}=1$.

%The similarity between two pixels i,j is determined by the similarity of the intensity gray level vectors $v(N_i)$ \& $v(N_j)$ of square neighborhoods of fixed size. This similarity index is defined by the weighted Euclidean distance $\parallel v(N_i )-v(N_j ) \parallel _{(2,a)}^2$  where $a > 0$ is the Euclidean distance between the two noisy neighborhoods, making the system robust:

%\begin{equation}
%	E\parallel v(N_i )-v(N_j )\parallel_{(2,a)}^2=
 %   \parallel u(N_i )-u(N_j ) \parallel _{2,a}^2+2 \sigma^2.
%\end{equation}

%The weights are defined as 
%\begin{equation}
%w(i,j)=\frac{1}{Z(i)} e^\frac{-\parallel v(N_i)-u(N_j) \parallel _{2,a}^2 }{h^2},
%\end{equation}

%where $Z(i)$ is the normalizing constant 

%\begin{equation}
%Z(i)=\sum_{j}{e^{\frac{-\parallel v(N_i)-u(N_j) \parallel _{2,a}^2 }{h^2}}}.
%\end{equation}

%The advantage of NLM denoising is that it not only compares the gray level with a single pixel but with the geometrical configuration of the neighborhood. 

\subsection{\acf{obnlm} filtering}

This method dedicated to ultrasound images is a modified version of the \ac{nlm} technique~\cite{obnlm}, to cope with the features of the speckle noise.
The \ac{nlm} algorithm analyses the patterns around the pixels rather than comparing intensity values which may be highly corrupted by noise.
For each pixel, the patches from the whole image are compared to find restoration parameters.
The modified algorithm proposes a Bayesian formulation of the \ac{nlm} filter inspired by~\cite{bayesian} which optimises the computational cost of the original algorithm. 

\subsection{\acf{pgpd}}
\ac{pgpd} is a recent patch-based technique proposed by Xu~\emph{et~al.}~\cite{xu2015patch}.
This method relies on a two-stage framework: (i) a learning stage and (ii) a denoising stage.
In the learning stage, non-local self-similar patches are grouped together and later modelled using a \ac{gmm}.
Then, for each component of the \ac{gmm}, a sparse dictionary is learnt.
In the denoising stage, the most suitable Gaussian component is selected with its corresponding dictionary and later used to denoise the image through a simple weighted sparse coding model.